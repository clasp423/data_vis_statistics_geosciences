{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Six - Space Weather \n",
    "\n",
    "## *Solar Impacts on Geomagnetic Storms*\n",
    "\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "- Perform linear regression and error analysis (part two).\n",
    "- Share an x-axis on plots.\n",
    "- Utilize the Pearson linear correlation coefficent (association). \n",
    "- Find the uncertainty in association using Bootstrap.\n",
    "- Explore different plotting styles.\n",
    "\n",
    "By the end of this lab you should be able to: analyze linear dependencies and calculate their uncertainty. \n",
    "\n",
    "Additional materials for reading: \n",
    "\n",
    "- Lecture Notes - Lectures 5 & 6\n",
    "- Igual & Segu√≠ Chapter 5 & 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Brief Introduction to Space Weather\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What IS space weather? \n",
    "\n",
    "\"Space-weather events are naturally occurring phenomena that have the potential to disrupt electric power systems; satellite, aircraft, and spacecraft operations; telecommunications; position, navigation, and timing services; and other technologies and infrastructures...\" Source - [National Space Weather Action Plan](https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/final_nationalspaceweatheractionplan_20151028.pdf)\n",
    "\n",
    "\"Space weather refers to the environmental conditions in Earth's magnetosphere (e.g. magnetic environment), ionosphere and thermosphere (e.g. upper atmosphere) due to the Sun and the solar wind that can influence the functioning and reliability of spaceborne and ground-based systems and services or endanger property or human health.\" Source - [European Space Agency](http://swe.ssa.esa.int/what-is-space-weather)\n",
    "\n",
    "In this lesson we learn about space weather and how the Sun - through the solar surface, magnetic fields, and particles - impacts the environment around Earth. A commonly known phenomena as a part of space weather is the aurora. \n",
    "\n",
    "<img src=\"./Images/AuroraNASA_Earth.jpg\" alt=\"Drawing1\" width=\"400px\"/><img src=\"./Images/AuroraNASA.jpg\" alt=\"Drawing2\" width=\"400px\"/>\n",
    "\n",
    "Sources: [NASA](https://www.nasa.gov/mission_pages/sunearth/news/gallery/HugoLAchre-20121010.html) & [NASA](https://www.nasa.gov/image-feature/aurora-and-the-pacific-northwest) \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Images/SpaceWeatherNOAA.jpg\" alt=\"SpaceWeatherInfographic\" width=\"800px\"/>\n",
    "\n",
    "Source: [NOAA](http://www.noaa.gov/explainers/space-weather-storms-from-sun)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Space Weather Impacts on Earth\n",
    "\n",
    "Go to your images folder and open ./Example_GeoSpaceWeather.mp4 \n",
    "\n",
    "Here we see a solar event (magnetic field and particles) impacting the Earth's magnetosphere (magnetic environment) resulting in aurora. Let's watch this together. \n",
    "\n",
    "Source: [NASA](https://svs.gsfc.nasa.gov/20097)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Part B.1 - Looking at Solar Dynamic Observatory (SDO) Data and Solar Wind Data*\n",
    "\n",
    "The Sun is dynamic, at certain periods in time it can be either more active (solar maximum), or less active (solar minimum). SDO has been taking images and other data of the Sun since ~ 2010.\n",
    "\n",
    "These data are from https://sdo.gsfc.nasa.gov/\n",
    "\n",
    "\n",
    "\n",
    "### Please explore under ./Data/ the videos of the Sun under QuietTime and ActiveTime subfolders. Compare the same colors to each other, what do we observe? Work with your neighbor for 2 minutes to review these videos and come up with one observation. \n",
    " \n",
    " -\n",
    " \n",
    " -\n",
    " \n",
    " -\n",
    " \n",
    " ### Part B.2 Let's now take a look and see what these solar images can tell us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages for this lab.\n",
    "\n",
    "import numpy as np                              #for accuracy \n",
    "import datetime as dt                           #for datetime objects\n",
    "import matplotlib.pyplot as plt                 #for pretty plotting\n",
    "import pandas as pd                             #for data structures\n",
    "from scipy import stats                         #for linear regression\n",
    "import matplotlib.dates as mdt                  #for manipulation of dates in matplotlib\n",
    "from matplotlib.ticker import MultipleLocator   #for pretty plotting\n",
    "import matplotlib.image as mpimg                #for manipulation of img files\n",
    "                                                #in plotting environments\n",
    "import importlib                                #for reimporting libaries / package\n",
    "import linerror as lr                           #same as Lab6 - for calculating linear\n",
    "                                                #fits\n",
    "        \n",
    "from matplotlib.patches import Rectangle        #NEW IN THIS LAB\n",
    "                                                #for plotting rectangles\n",
    "    \n",
    "import omni_data_functions as omni_fn           #NEW IN THIS LAB\n",
    "                                                #for easy importing\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix    #NEW IN THIS LAB\n",
    "                                                #for confusion matrix\n",
    "                                                #note sklearn is an extensive \n",
    "                                                #package with a lot of functionality\n",
    "                                                #particularly for machine learning\n",
    "                                                #and high end data analytics\n",
    "#for plotting in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will start with something simple - let's plot the solar data we saw from SDO as an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image files.\n",
    "quiet_sun = mpimg.imread('./Data/SolarMinImages/20180105_022906_512_0304.jpg')\n",
    "active_sun = mpimg.imread('./Data/SolarMaxImages/20130317_044532_512_0304.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a 1x2 matplotlib plot with solar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7))\n",
    "fig.suptitle('The Sun Taken From Solar Dynamics Observatory - 512 $\\AA$', fontsize=20)\n",
    "\n",
    "gs = plt.GridSpec(1, 2, hspace=0.1, wspace=0.2, right = 0.9)\n",
    "\n",
    "#add subplots\n",
    "ax1 = fig.add_subplot(gs[:,0])\n",
    "ax2 = fig.add_subplot(gs[:,1])\n",
    "\n",
    "#make aspect ratio better - 1.0 is square. Feel free to play around with these\n",
    "#and see how it streches.\n",
    "ax1.set_aspect(1.0)\n",
    "ax2.set_aspect(1.0)\n",
    "\n",
    "#plot images data - note for now use imshow to plot out the 3D datasets\n",
    "ax1.imshow(active_sun)\n",
    "ax2.imshow(quiet_sun)\n",
    "\n",
    "#setting labeling\n",
    "ax1.set_xlabel('Taken on 2013/03/17 04:45:32 UT',   fontsize = 20)\n",
    "ax2.set_xlabel('Taken on 2018/1/05 02:29:06 UT', fontsize = 20)\n",
    "\n",
    "# Removing ticks and tick labels.\n",
    "ax1.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "ax2.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part B.3 Reviewing Near-Earth Data at These Times\n",
    " \n",
    "We have near-Earth data from the space environment at these same times. This data is from the OMNI site - https://omniweb.gsfc.nasa.gov/html/ow_data.html#1\n",
    "\n",
    "\"primarily a 1963-to-current compilation of hourly-averaged, near-Earth solar \n",
    "wind magnetic field and plasma parameter data from several spacecraft in geocentric or \n",
    "L1 (Lagrange point) orbits. The data have been extensively cross compared, and, \n",
    "for some spacecraft and parameters, cross-normalized. Time-shifts of higher \n",
    "resolution data to expected magnetosphere-arrival times are done for data from \n",
    "spacecraft in L1 orbits (ISEE 3, Wind, ACE), prior to taking hourly averages.\"\n",
    "\n",
    "**Let's read this in now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['YEAR', 'DOY', 'Hour', 'BX', 'BY', 'BZ', 'FlowPressure', 'Ey', 'Kp', \n",
    "            'SunspotNumber', 'Dst', 'f10.7_index']\n",
    "\n",
    "hour_omni = pd.read_csv('./Data/omni2_Hourly1980_2018.lst', sep = '\\s+', names = column_names,\n",
    "                          parse_dates = {'Datetime': column_names[0:3]}, keep_date_col = 'True')\n",
    "\n",
    "hour_omni.index = pd.to_datetime(hour_omni['Datetime'], infer_datetime_format = False, \n",
    "                                   format = '%Y %j %H')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity for you! Look at the first two rows of hour_omni and the last row - what do we notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your Code Here\n",
    "\n",
    "print(?)\n",
    "print(?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "Let's fix these missing values in our dataset. Missing values are 9's in whatever format is specified in the read me from\n",
    "the data site. Because the data has different format - we replace the similarly formatter columns with each of the retrospective nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_omni[['BX', 'BY', 'BZ', 'f10.7_index']] = hour_omni[[\n",
    "    'BX', 'BY', 'BZ', 'f10.7_index']].replace(to_replace = 999.9, value = np.nan)\n",
    "\n",
    "hour_omni['FlowPressure'] = hour_omni['FlowPressure'].replace(\n",
    "    to_replace = 99.99, value = np.nan)\n",
    "\n",
    "hour_omni['Ey'] = hour_omni['Ey'].replace(to_replace = 999.99, value = np.nan)\n",
    "\n",
    "hour_omni[['SunspotNumber', 'Kp']] = hour_omni[['SunspotNumber', 'Kp']].replace(\n",
    "    to_replace = 999, value = np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've cleaned our data, let's compare the active and non-active times we were looking at before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and stop dates of active and non-active times.\n",
    "\n",
    "startdate_active = dt.datetime(2013, 3, 15)\n",
    "stopdate_active  = dt.datetime(2013, 3, 20)\n",
    "\n",
    "idx_active =  (hour_omni.index > startdate_active) & (hour_omni.index < stopdate_active)\n",
    "\n",
    "# Similar for quiet solar times.\n",
    "startdate_quiet  = dt.datetime(2018, 1, 3)\n",
    "stopdate_quiet   = dt.datetime(2018, 1, 8)\n",
    "\n",
    "idx_quiet = (hour_omni.index > startdate_quiet) & (hour_omni.index < stopdate_quiet)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and now let's make our plot\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "fig.suptitle('OMNI Data During Selected Dates in Solar Max & Solar Min', fontsize=20)\n",
    "\n",
    "gs  = plt.GridSpec(4, 2, hspace=0.0, wspace=0.2, right = 0.9)\n",
    "\n",
    "#add subplots\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[1,0])\n",
    "ax3 = fig.add_subplot(gs[2,0])\n",
    "ax4 = fig.add_subplot(gs[3,0])\n",
    "\n",
    "ax5 = fig.add_subplot(gs[0,1])\n",
    "ax6 = fig.add_subplot(gs[1,1])\n",
    "ax7 = fig.add_subplot(gs[2,1])\n",
    "ax8 = fig.add_subplot(gs[3,1])\n",
    "\n",
    "\n",
    "\n",
    "#set up titles for ranges\n",
    "ax1.set_title('Solar Maximum', fontsize = 15)\n",
    "ax5.set_title('Solar Minimum', fontsize = 15)\n",
    "\n",
    "#set up titles for \n",
    "ax1.set_ylabel('Sunspot #', fontsize = 20)\n",
    "ax2.set_ylabel('F10.7', fontsize = 20)\n",
    "ax3.set_ylabel('E Field (y)', fontsize = 20)\n",
    "ax4.set_ylabel('D$_{st}$', fontsize = 20)\n",
    "\n",
    "\n",
    "#and plot and add colors / line width - \n",
    "\n",
    "#set up list of axes objects\n",
    "axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\n",
    "\n",
    "#set up list labels\n",
    "labels = ['SunspotNumber', 'f10.7_index', 'Ey', 'Dst']\n",
    "#set up list of colors\n",
    "colors = ['#3b5998', '#987a3b', '#008287', '#983b59']\n",
    "\n",
    "\n",
    "#loop over axes, colors and labels to produce multiple\n",
    "#subplots\n",
    "for ax, label, c in zip(axes[0:4], labels, colors):\n",
    "    \n",
    "    ax.plot(hour_omni.loc[idx_active, :].index.to_pydatetime(), \n",
    "         hour_omni.loc[idx_active, label], color = c, lw = 4.0)\n",
    "    \n",
    "for ax, label, c in zip(axes[4:9], labels, colors):\n",
    "\n",
    "    ax.plot(hour_omni.loc[idx_quiet, :].index.to_pydatetime(), \n",
    "         hour_omni.loc[idx_quiet, label], color = c, lw = 4.0)\n",
    "\n",
    "    \n",
    "#set up the labels to designate the time ranges\n",
    "ax4.set_xlabel('From {:%m/%d} to {:%m/%d %Y} UT'.format(startdate_active, \n",
    "                stopdate_active), fontsize = 20)\n",
    "ax8.set_xlabel('From {:%m/%d} to {:%m/%d %Y} UT'.format(startdate_quiet, \n",
    "                stopdate_quiet), fontsize = 20)\n",
    "\n",
    "#SET UP FORMAT\n",
    "ax4.xaxis.set_major_locator(mdt.DayLocator())\n",
    "ax4.xaxis.set_major_formatter(mdt.DateFormatter('%d'))\n",
    "\n",
    "ax8.xaxis.set_major_locator(mdt.DayLocator())\n",
    "ax8.xaxis.set_major_formatter(mdt.DateFormatter('%d'))\n",
    "\n",
    "#setting tick parameters to null - these are just pixel values so in this case, not\n",
    "#incredibly informative\n",
    "ax1.set_xticklabels([]) #make the tick labels small in overlap\n",
    "ax2.set_xticklabels([])\n",
    "ax3.set_xticklabels([])\n",
    "\n",
    "ax5.set_xticklabels([]) #make the tick labels small in overlap\n",
    "ax6.set_xticklabels([])\n",
    "ax7.set_xticklabels([])\n",
    "\n",
    "\n",
    "#set axis ranges\n",
    "ax1.set_ylim([-5, 130])\n",
    "ax5.set_ylim([-5, 130])\n",
    "\n",
    "ax2.set_ylim([65, 135])\n",
    "ax6.set_ylim([65, 135])\n",
    "\n",
    "ax3.set_ylim([-10, 12])\n",
    "ax7.set_ylim([-10, 12])\n",
    "\n",
    "ax4.set_ylim([-150, 30])\n",
    "ax8.set_ylim([-150, 30])\n",
    "\n",
    "#this is repetitive - to set each one so we make a for loop\n",
    "for ax in axes: \n",
    "    ax.grid(color='gray', linestyle='dashed')\n",
    "    ax.tick_params(labelsize = 14)\n",
    "    \n",
    "\n",
    "#set up zero line \n",
    "ax3.axhline(y=0.0, color='k', linestyle='--') #add a horizontal line at 0\n",
    "ax4.axhline(y=0.0, color='k', linestyle='--') \n",
    "ax7.axhline(y=0.0, color='k', linestyle='--') \n",
    "ax8.axhline(y=0.0, color='k', linestyle='--');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From looking at these plots what can we say about the quiet and non quiet times? Do we think anything is correlated to each other based on this quick look plot? Take a minute with a neighbor to discuss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Part C - Looking at Longer Term Solar Cycle Data*\n",
    "\n",
    "So far we've seen two examples of a few hours and snapshots of the solar max and solar min - what does this look like over time? \n",
    "\n",
    "Let's download some lower resolution time data - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download daily and monthly resolution data - this is from the same\n",
    "# omni site as before but this is now at different (lower) resolution.\n",
    "\n",
    "daily_omni = omni_fn.load_omni_data('./Data/omni2_Daily1980_2018.lst')\n",
    "monthly_omni = omni_fn.load_omni_data('./Data/omni2_27Day1980_2018.lst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again you should always check your data that you import. In the below cell check out what we just read into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some checking data lines of code here.\n",
    "\n",
    "??\n",
    "??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the long duration trend of some of those values from before. \n",
    "\n",
    "#### This is an alternative if you have different scales of data but would like them to be still on the same plot without a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "fig.suptitle('Long Duration Solar Behavior', fontsize=20)\n",
    "\n",
    "gs  = plt.GridSpec(1, 1, hspace=0.0, wspace=0.2, right = 0.85)\n",
    "\n",
    "#add subplots\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "\n",
    "\n",
    "#this ties together ax1 and ax2 - \n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#set up titles for axis\n",
    "ax1.set_ylabel('Number of Sunspots', fontsize = 20, color = 'grey')\n",
    "ax2.set_ylabel('f10.7 Standard Flux Units (sfu)', fontsize = 20, \n",
    "               color = '#983b59', rotation = 270, labelpad = 30)\n",
    "\n",
    "ax1.plot(daily_omni.index.to_pydatetime(), daily_omni['SunspotNumber'], \n",
    "         color = 'grey', lw = 2.0)\n",
    "\n",
    "ax2.plot(daily_omni.index.to_pydatetime(), daily_omni['f10.7_index'], \n",
    "         color = '#983b59', lw = 1.0, alpha = 0.8)\n",
    "    \n",
    "ax1.set_xlabel('Years', fontsize = 20)\n",
    "\n",
    "#SET UP FORMAT\n",
    "ax1.xaxis.set_major_locator(mdt.YearLocator(5))\n",
    "ax1.xaxis.set_major_formatter(mdt.DateFormatter('%Y'))\n",
    "\n",
    "ax1.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "#how to set up a share axis with alternative colors\n",
    "ax1.tick_params('y', colors = 'grey', labelsize = 16)\n",
    "ax2.tick_params('y', colors = '#983b59', labelsize = 16)\n",
    "\n",
    "#set label size of the x axis\n",
    "ax1.tick_params('x', labelsize = 16)\n",
    "\n",
    "#nudge it up a bit \n",
    "ax2.set_ylim([25, 400])\n",
    "\n",
    "plt.savefig('./Figures/LongDurationSolar.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we observe from this chart? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Part C.1 - Linear Regression of Solar Cycle Data*\n",
    "\n",
    "Given the above figure - let's now compare and constrast the two y-variables we had and instead plot 10.7 vs sunspot number - do we think this would be linear? In the last lab we performed linear regression along with the error values along this line. \n",
    "\n",
    "Let's repeat that analysis but add in the Pearson correlation coefficent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this doesn't work on nan values first we remove all potential nans.\n",
    "\n",
    "nan_mask = ((~np.isnan(daily_omni['SunspotNumber'])) & \n",
    "           (~np.isnan(daily_omni['f10.7_index'])))\n",
    "\n",
    "x_vals_ss_f107 = daily_omni.loc[nan_mask, 'SunspotNumber']\n",
    "y_vals_ss_f107 = daily_omni.loc[nan_mask, 'f10.7_index']\n",
    "\n",
    "#Run our linear regression\n",
    "slope, intercept, rval, pval, stderr = stats.linregress(x_vals_ss_f107, y_vals_ss_f107)\n",
    "\n",
    "\n",
    "#calculate the yvalues given the linear fit\n",
    "y_model_sunspots_f107 = intercept + slope * x_vals_ss_f107\n",
    "\n",
    "\n",
    "#find y errors\n",
    "rmse  = lr.calc_rmse(y_model_sunspots_f107, y_vals_ss_f107)\n",
    "\n",
    "#error on coefficents, slope and y-intercept and.\n",
    "slope_error,  intercept_error = lr.calc_coeffs_error(x_vals_ss_f107,  rmse)\n",
    "\n",
    "\n",
    "#print out fit report - \n",
    "print(\"Fit Report: \\n \\tUncert. on Y: +/- {:.2f}\".format(rmse) + \n",
    "      \"\\n \\tIntercept: {:.2f} +/- {:.2f}\".format(intercept, intercept_error)\n",
    "      + \"\\n\\tSlope: {:.2f} +/- {:.2f}\".format(slope, slope_error) \n",
    "      + \"\\n\\tPearson linear correlation: {:.2f}, r-squared: {:.2f}\".format(rval, rval**2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we think this is linear? What does this report tell us? Note we now have an r-squared value. What does this tell us about the fit even before plotting this?\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "### Let's take a look at that fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make plot of fit\n",
    "\n",
    "fig = plt.figure(figsize=(8.5, 5))\n",
    "fig.suptitle('Dependence of f10.7 on Number of Sunspots', fontsize=20)\n",
    "\n",
    "gs  = plt.GridSpec(1, 1, hspace=0.0, wspace=0.2, right = 0.9)\n",
    "\n",
    "#add subplots\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "\n",
    "#set up titles for axis\n",
    "ax1.set_xlabel('Number of Sunspots', fontsize = 20)\n",
    "ax1.set_ylabel('f10.7 Standard Flux Units (sfu)', fontsize = 20)\n",
    "\n",
    "#plot values\n",
    "ax1.scatter(x_vals_ss_f107, y_vals_ss_f107,  color = '#544cac', alpha = 0.6, s = 2)\n",
    "ax1.plot(x_vals_ss_f107, y_model_sunspots_f107, color = '#edb12e', alpha = 0.6, lw = 3)\n",
    "\n",
    "#set up grid\n",
    "ax1.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "#how to set up a share axis with alternative colors\n",
    "ax1.tick_params(labelsize = 16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C.2: Bootstrap Method\n",
    "\n",
    "In this part of the lab we will implement the bootstrap method we learned about in Lecture 6 on Monday. \n",
    "\n",
    "The bootstrap method is where you repeatedly resample a dataset with replacement, i.e. you can have repeat values, to gain an estimation of error.\n",
    "\n",
    "## Take 3 minutes to chat with a neighbor. What might be some reasons we might want to resample a dataset and do a bootstrap analysis to gain an estimation of error? Recall this is beyond just the error on the y-intercept, the root mean square error, or the slope values.\n",
    "\n",
    "### See below for the example within the class lecture notes. \n",
    "<img src=\"./Images/ExampleBootstrap.png\" alt=\"ExampleBootstrapImage\" height=\"400px\" width=\"350px\"/>\n",
    "\n",
    "\n",
    "---- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that the number of sunspots and f10.7 are highly linear when plotted against each other and analyzed. \n",
    "\n",
    "### What about looking at the f10.7 and the Dst? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since this doesn't work on nan values first we remove all potential nans' \n",
    "nan_mask = ((~np.isnan(monthly_omni['f10.7_index'])) & \n",
    "           (~np.isnan(monthly_omni['Dst'])))\n",
    "\n",
    "x_vals = monthly_omni.loc[nan_mask, 'f10.7_index']\n",
    "y_vals = monthly_omni.loc[nan_mask, 'Dst']\n",
    "\n",
    "\n",
    "#calculate the linear fit\n",
    "slope_orig, intercept_orig, rval_orig, pval_orig, stderr_orig = stats.linregress(x_vals, y_vals)\n",
    "\n",
    "#calculate the yvalues given the linear fit\n",
    "ymodel_orig = intercept_orig + slope_orig * x_vals\n",
    "\n",
    "\n",
    "#find y errors\n",
    "rmse  = lr.calc_rmse(ymodel_orig, y_vals)\n",
    "\n",
    "#error on coefficents, slope and y-intercept and.\n",
    "slope_error,  intercept_error = lr.calc_coeffs_error(x_vals,  rmse)\n",
    "\n",
    "\n",
    "#print out fit report - \n",
    "print(\"Fit Report: \\n \\tUncert. on Y: +/- {:.2f}\".format(rmse) + \n",
    "      \"\\n \\tIntercept: {:.2f} +/- {:.2f}\".format(intercept_orig, intercept_error)\n",
    "      + \"\\n\\tSlope: {:.2f} +/- {:.2f}\".format(slope_orig, slope_error) \n",
    "      + \"\\n\\tPearson linear correlation: {:.2f}, r-squared: {:.2f}\".format(rval_orig, rval_orig**2))\n",
    "\n",
    "\n",
    "#make plot\n",
    "fig = plt.figure(figsize=(8.5, 5))\n",
    "fig.suptitle('Dependence of D$_{st}$ on f10.7 by Month', fontsize=20)\n",
    "\n",
    "gs  = plt.GridSpec(1, 1, hspace=0.0, wspace=0.2, right = 0.9)\n",
    "\n",
    "#add subplots\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "\n",
    "#set up titles for axis\n",
    "ax1.set_xlabel('f10.7 Standard Flux Units (sfu)', fontsize = 20)\n",
    "ax1.set_ylabel('D$_{st}$ (nT)', fontsize = 20)\n",
    "\n",
    "#plot values\n",
    "ax1.scatter(x_vals, y_vals,  color = '#660066', alpha = 0.6, s = 20)\n",
    "ax1.plot(x_vals, ymodel_orig, color = '#006666', alpha = 1.0, linestyle = '--', lw = 3)\n",
    "\n",
    "#set up grid\n",
    "ax1.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "#how to set up a share axis with alternative colors\n",
    "ax1.tick_params(labelsize = 16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what is the estimation on the correlation coefficent (Pearson linear correlation)? Here we need to set up our bootstrap analysis.\n",
    "\n",
    "### Bootstrap Method:\n",
    "\n",
    "### Step 1 - Resample our pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.sample is a method of dataframes - \n",
    "\n",
    "#the parameter frac means % of the dataframe we want in the sampling\n",
    "resample = monthly_omni[['f10.7_index', 'Dst']].sample(frac=1, replace=True)\n",
    "\n",
    "\n",
    "print(resample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And redo the fit for our newly resampled data frame\n",
    "\n",
    "nan_mask = ((~np.isnan(resample['f10.7_index'])) & \n",
    "           (~np.isnan(resample['Dst'])))\n",
    "\n",
    "x_vals = resample.loc[nan_mask, 'f10.7_index']\n",
    "y_vals = resample.loc[nan_mask, 'Dst']\n",
    "\n",
    "slope, intercept, rval, pval, stderr = stats.linregress(x_vals, y_vals)\n",
    "\n",
    "#calculate the yvalues given the linear fit\n",
    "y_model = intercept + slope * x_vals\n",
    "\n",
    "\n",
    "#find y errors\n",
    "rmse  = lr.calc_rmse(y_model, y_vals)\n",
    "\n",
    "#error on coefficents, slope and y-intercept and.\n",
    "slope_error,  intercept_error = lr.calc_coeffs_error(x_vals,  rmse)\n",
    "\n",
    "\n",
    "#print out fit report - \n",
    "print(\"Fit Report: \\n \\tUncert. on Y: +/- {:.2f}\".format(rmse) + \n",
    "      \"\\n \\tIntercept: {:.2f} +/- {:.2f}\".format(intercept_orig, intercept_error)\n",
    "      + \"\\n\\tSlope: {:.2f} +/- {:.2f}\".format(slope, slope_error) \n",
    "      + \"\\n\\tPearson linear correlation: {:.2f}, r-squared: {:.2f}\".format(rval, rval**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a second and compare this to the fit you ran before...does this look a bit different or similar? Compare to your neighbor, is it the same as theirs?\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "### Step 2: Implementing Repetition \n",
    "\n",
    "Let's put this into a loop and resample for many values - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resample and calculate the fit many times - this\n",
    "# will take a while to run - be patient. \n",
    "\n",
    "# As suggested in class 500 would be appropiate. \n",
    "num_iterations = 500\n",
    "\n",
    "# create dictionary of bootstrap values\n",
    "bootstrap_vals = {\n",
    "                  'r_vals': np.zeros(num_iterations),\n",
    "                  'intercepts' : np.zeros(num_iterations),\n",
    "                  'slopes' : np.zeros(num_iterations)\n",
    "                  }\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    #resample the dataframe\n",
    "    resample = monthly_omni[['Dst', \n",
    "                           'f10.7_index']].sample(frac = 1, replace = True)\n",
    "    \n",
    "    #create nan mask\n",
    "    nan_mask = ((~np.isnan(resample['f10.7_index'])) & \n",
    "               (~np.isnan(resample['Dst'])))\n",
    "\n",
    "    #new values of x, and y without nans\n",
    "    x_vals = resample.loc[nan_mask, 'f10.7_index']\n",
    "    y_vals = resample.loc[nan_mask, 'Dst']\n",
    "\n",
    "    #Perform fit.\n",
    "    #Notice the underscore. \n",
    "    #This is just a placeholder for return values from our function\n",
    "    # that we are not going to use. The get returned but not\n",
    "    # assigned to any variable (meaning that we cannot use them,\n",
    "    # but here we are saying that we don't need to use them)\n",
    "    slope, intercept, rval, *_ = stats.linregress(x_vals, y_vals)\n",
    "    \n",
    "    # populate dictionay with the values\n",
    "    bootstrap_vals['r_vals'][i] = rval\n",
    "    \n",
    "    bootstrap_vals['intercepts'][i] = intercept\n",
    "    \n",
    "    bootstrap_vals['slopes'][i] = slope\n",
    "\n",
    "#print out final values\n",
    "print(\"The mean r value is {:.2f} with a standard deviation of {:.5f}.\".format(\n",
    "    np.mean(bootstrap_vals['r_vals']), \n",
    "    np.std(bootstrap_vals['r_vals'], ddof = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluation of the Fit\n",
    "\n",
    "Now let's take a look at these values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8.5, 5))\n",
    "fig.suptitle('D$_{st}$ on f10.7 by Month, ' + \n",
    "             'Resampling Amount: {} \\n'.format(num_iterations), \n",
    "             fontsize=20)\n",
    "\n",
    "gs  = plt.GridSpec(1, 1, hspace=0.0, wspace=0.2, right = 0.9)\n",
    "\n",
    "#add subplots\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "\n",
    "#set up titles for axis\n",
    "ax1.set_xlabel('f10.7 Standard Flux Units (sfu)', fontsize = 20)\n",
    "ax1.set_ylabel('D$_{st}$ (nT)', fontsize = 20)\n",
    "\n",
    "#plot values\n",
    "ax1.scatter(monthly_omni['f10.7_index'], monthly_omni['Dst'],  \n",
    "            color = '#660066', alpha = 0.6, s = 20)\n",
    "\n",
    "#make line equally spaced\n",
    "minimum_value  = np.nanmin(monthly_omni['f10.7_index'])\n",
    "max_value  = np.nanmax(monthly_omni['f10.7_index'])\n",
    "spacing = (max_value - minimum_value) / 20.0\n",
    "\n",
    "#create new xarray for pretty plotting\n",
    "x_vals = np.arange(minimum_value, max_value+spacing, int(spacing))\n",
    "\n",
    "for slope, intercept in zip(bootstrap_vals['slopes'], bootstrap_vals['intercepts']):\n",
    "    \n",
    "    #calculate y values\n",
    "    y_model = slope*x_vals + intercept \n",
    "    \n",
    "    #and plot in gray\n",
    "    ax1.plot(x_vals, y_model, color = 'grey', alpha = 0.2, lw = 1.0)\n",
    "    \n",
    "#plot final value in turquoise - \n",
    "ymodel_orig = slope_orig*x_vals + intercept_orig\n",
    "\n",
    "ax1.plot(x_vals, ymodel_orig, \n",
    "         color = '#006666', alpha = 1.0, linestyle = '--', lw = 3.0)\n",
    "\n",
    "\n",
    "#set up grid\n",
    "ax1.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "#set up label sizes\n",
    "ax1.tick_params(labelsize = 16)\n",
    "\n",
    "# dpi stands for dots per inch, and sets the resolution\n",
    "# of the saved figure file. dpi=300 is usually \n",
    "# a good resolution for most applications.\n",
    "plt.savefig('./Figures/bootstrap_linear_fits.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at a distribution of these r values. \n",
    "\n",
    "**What does the histogram of this look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the figure \n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "fig.suptitle('Histogram of Pearson Corr. Coeff. for D$_{st}$ vs. f10.7 \\n' + \n",
    "             'Resampling Amount: {}'.format(num_iterations), \n",
    "             fontsize=20)\n",
    "\n",
    "\n",
    "#minimum as the sqrt(sample_size)\n",
    "sample_size = num_iterations\n",
    "\n",
    "\n",
    "num_bins = int(np.ceil(np.sqrt(sample_size)))\n",
    "print(\"The number of bins for the histogram is: {}\".format(num_bins))\n",
    "\n",
    "#edgecolor and linewidth set up the bin edges\n",
    "ax.hist(bootstrap_vals['r_vals'], num_bins, density = False, facecolor = '#822f59', \n",
    "         edgecolor=\"k\")\n",
    "\n",
    "#set up grid\n",
    "ax.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "#labels\n",
    "ax.set_xlabel('Pearson Correlation Coeff.', fontsize = 20)\n",
    "ax.set_ylabel('Occurrence', fontsize = 20)\n",
    "#large ticks\n",
    "plt.xticks(fontsize=16) #make the xaxis labels larger\n",
    "plt.yticks(fontsize=16) #make the yaxis labels larger\n",
    "\n",
    "#print out stats on the skew etc\n",
    "print(\"Skew: {:.3f}, Kurtosis: {:.3f}, Standard Dev: {:.3f}, Mean: {:.3f}\".format(\n",
    "        stats.skew(bootstrap_vals['r_vals']), \n",
    "        stats.kurtosis(bootstrap_vals['r_vals'], fisher = False), \n",
    "        np.std(bootstrap_vals['r_vals'], ddof = 1), \n",
    "        np.mean(bootstrap_vals['r_vals'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this chart inform you about the bootstrap analysis?\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "### Finally we calculate the t-value for this comparison - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how does the z value look?\n",
    "t_val = (abs((rval_orig - np.mean(bootstrap_vals['r_vals']))) / \n",
    "        (np.std(bootstrap_vals['r_vals'], ddof = 1)))\n",
    "\n",
    "print(\"The t-test results in a comparison\" + \n",
    "      \" of the original fit to the bootstrap fit of {:.5f}.\".format(t_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this a small or a large t-value? What does that mean about our fit? \n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use our t-value to determine the probability that we would get the first r value by chance alone.\n",
    "\n",
    "**There is a scipy function for that. No more tables!** (Unless you really like using tables.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within `scipy.stats`, there is a `t` distribution, and the `t` distribution has a probability density function. The t-test chart that you know already is based on the *cumulative* density function. The cell below shows its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = stats.t.cdf(x=t_val, df=num_iterations-1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we interpret this probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In lab Activity\n",
    "## Practice using the bootstrap method.\n",
    "\n",
    "#### Now we will use the bootstrap method to calculate the 'best' r, and its uncertainty for the linear correlation between sunspot number and F10.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like before:\n",
    "num_iterations = ??\n",
    "\n",
    "# create dictionary of bootstrap values\n",
    "bootstrap_vals = {\n",
    "                  'r_vals': np.zeros(num_iterations),\n",
    "                  'intercepts' : np.zeros(num_iterations),\n",
    "                  'slopes' : np.zeros(num_iterations)\n",
    "                  }\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    #resample the dataframe using SunspotNumber and f10.7_index columns\n",
    "    resample = daily_omni[??].sample(frac=?, replace=?)\n",
    "    \n",
    "    #create nan mask    \n",
    "    nan_mask = ??\n",
    "    \n",
    "    \n",
    "    #new values of x, and y without nans\n",
    "    x_vals = ?? # sunspot numbers for x\n",
    "    y_vals = ?? # f10.7 for y\n",
    "\n",
    "\n",
    "    #Perform fit. What is *_ ?\n",
    "    slope, intercept, rval, *_ = ??\n",
    "    \n",
    "    # populate dictionay with the values\n",
    "    bootstrap_vals['r_vals'][?] = ??\n",
    "    \n",
    "    bootstrap_vals['intercepts'][?] = ??\n",
    "    \n",
    "    bootstrap_vals['slopes'][?] = ??\n",
    "\n",
    "#print out final values\n",
    "print(\"The mean r value is {:.5f} with a standard deviation of {:.5f}.\".format(\n",
    "    np.mean(bootstrap_vals['r_vals']), \n",
    "    np.std(bootstrap_vals['r_vals'], ddof = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a figure, similar to before showing all of our calculated lines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here for plotting...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "In this lab we have learned a little bit about space weather data, how to calculate the correlation coefficient (r), and how to use a bootstrap method to calculate an uncertainty on r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Bonus - Time Saving for Plotting*\n",
    "\n",
    "### What if we wanted to implement this onto *any* dataset without copy paste? Let's learn some time saving tools here. \n",
    "\n",
    "If you just want to run forward with some quick exploratory plots - I reccomend Seaborn. Please note it's difficult to obtain finalized plots in seaborn with what we've learned so far but I support using these quick look tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns #this is a new package here\n",
    "                      #I like to think of this as a stats + visualization package\n",
    "    \n",
    "#makes a quick plot AND linear fit\n",
    "#runs nicely with pandas dataframes\n",
    "sns.regplot(x = 'f10.7_index', y = 'Dst', data = monthly_omni, n_boot = 1000);\n",
    "\n",
    "#note the confidence interval drawn around the dataset \n",
    "#dependent on the bootstrap via n_boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also get the same \"seaborn style\" with some nifty visualization tricks...such as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.style.use('fivethirtyeight')\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(8.5, 5))\n",
    "\n",
    "#set up titles for axis\n",
    "ax.set_xlabel('f10.7 Standard Flux Units (sfu)', fontsize = 20)\n",
    "ax.set_ylabel('D$_{st}$ (nT)', fontsize = 20)\n",
    "\n",
    "#plot values\n",
    "x_vals = monthly_omni['f10.7_index']\n",
    "y_vals = monthly_omni['Dst']\n",
    "ax.scatter(x_vals, y_vals)\n",
    "\n",
    "\n",
    "ax.tick_params(labelsize = 16)\n",
    "\n",
    "\n",
    "print(\"The following styles are availible:\")\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The great thing about these is that you can use some of the defaults to set fontsizes etc, each time you plot!\n",
    "\n",
    "You can find more about this here - https://matplotlib.org/tutorials/introductory/customizing.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
